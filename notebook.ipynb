{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Detection for Customer Intent Classification\n",
    "\n",
    "## Resources\n",
    "\n",
    "Datasets \n",
    "- [banking77](https://huggingface.co/datasets/banking77)\n",
    "- [ATIS Airline Travel Information System](https://www.kaggle.com/datasets/hassanamin/atis-airlinetravelinformationsystem) \n",
    "- [Bitext - Customer Service Tagged Training Dataset for Intent Detection](https://github.com/bitext/customer-support-intent-detection-training-dataset)\n",
    "\n",
    "Model\n",
    "- [philschmid/BERT-Banking77](https://huggingface.co/philschmid/BERT-Banking77) \n",
    "\n",
    "BERT resources\n",
    "- [BERT Neural Network - EXPLAINED!](https://www.youtube.com/watch?v=xI0HHN5XKDo) \n",
    "- [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps)\n",
    "\n",
    "OOD detection\n",
    "- [`Todd`](https://github.com/icannos/Todd)\n",
    "- [OOD Classification `ToddBenchmark`](https://github.com/icannos/ToddBenchmark/tree/master/examples/classifications) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Todd import ScorerType, MahalanobisScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toddbenchmark.classification_datasets import (\n",
    "    prep_model, \n",
    "    load_b77, \n",
    ")\n",
    "from toddbenchmark.classification_datasets_configs import (\n",
    "    DATASETS_CONFIGS,\n",
    "    load_requested_dataset,\n",
    ")\n",
    "\n",
    "from toddbenchmark.utils_classification import (\n",
    "    prepare_detectors,\n",
    "    evaluate_dataloader,\n",
    ")\n",
    "\n",
    "from toddbenchmark.utils import mk_file_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config, model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\"BERT\": \"philschmid/BERT-Banking77\", \"DistilBERT\": \"philschmid/DistilBERT-Banking77\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"philschmid/BERT-Banking77\",\n",
       "  \"_num_labels\": 77,\n",
       "  \"architectures\": [\n",
       "    \"BertForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 1.000E-01,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 1.000E-01,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"Refund_not_showing_up\",\n",
       "    \"1\": \"activate_my_card\",\n",
       "    \"2\": \"age_limit\",\n",
       "    \"3\": \"apple_pay_or_google_pay\",\n",
       "    \"4\": \"atm_support\",\n",
       "    \"5\": \"automatic_top_up\",\n",
       "    \"6\": \"balance_not_updated_after_bank_transfer\",\n",
       "    \"7\": \"balance_not_updated_after_cheque_or_cash_deposit\",\n",
       "    \"8\": \"beneficiary_not_allowed\",\n",
       "    \"9\": \"cancel_transfer\",\n",
       "    \"10\": \"card_about_to_expire\",\n",
       "    \"11\": \"card_acceptance\",\n",
       "    \"12\": \"card_arrival\",\n",
       "    \"13\": \"card_delivery_estimate\",\n",
       "    \"14\": \"card_linking\",\n",
       "    \"15\": \"card_not_working\",\n",
       "    \"16\": \"card_payment_fee_charged\",\n",
       "    \"17\": \"card_payment_not_recognised\",\n",
       "    \"18\": \"card_payment_wrong_exchange_rate\",\n",
       "    \"19\": \"card_swallowed\",\n",
       "    \"20\": \"cash_withdrawal_charge\",\n",
       "    \"21\": \"cash_withdrawal_not_recognised\",\n",
       "    \"22\": \"change_pin\",\n",
       "    \"23\": \"compromised_card\",\n",
       "    \"24\": \"contactless_not_working\",\n",
       "    \"25\": \"country_support\",\n",
       "    \"26\": \"declined_card_payment\",\n",
       "    \"27\": \"declined_cash_withdrawal\",\n",
       "    \"28\": \"declined_transfer\",\n",
       "    \"29\": \"direct_debit_payment_not_recognised\",\n",
       "    \"30\": \"disposable_card_limits\",\n",
       "    \"31\": \"edit_personal_details\",\n",
       "    \"32\": \"exchange_charge\",\n",
       "    \"33\": \"exchange_rate\",\n",
       "    \"34\": \"exchange_via_app\",\n",
       "    \"35\": \"extra_charge_on_statement\",\n",
       "    \"36\": \"failed_transfer\",\n",
       "    \"37\": \"fiat_currency_support\",\n",
       "    \"38\": \"get_disposable_virtual_card\",\n",
       "    \"39\": \"get_physical_card\",\n",
       "    \"40\": \"getting_spare_card\",\n",
       "    \"41\": \"getting_virtual_card\",\n",
       "    \"42\": \"lost_or_stolen_card\",\n",
       "    \"43\": \"lost_or_stolen_phone\",\n",
       "    \"44\": \"order_physical_card\",\n",
       "    \"45\": \"passcode_forgotten\",\n",
       "    \"46\": \"pending_card_payment\",\n",
       "    \"47\": \"pending_cash_withdrawal\",\n",
       "    \"48\": \"pending_top_up\",\n",
       "    \"49\": \"pending_transfer\",\n",
       "    \"50\": \"pin_blocked\",\n",
       "    \"51\": \"receiving_money\",\n",
       "    \"52\": \"request_refund\",\n",
       "    \"53\": \"reverted_card_payment?\",\n",
       "    \"54\": \"supported_cards_and_currencies\",\n",
       "    \"55\": \"terminate_account\",\n",
       "    \"56\": \"top_up_by_bank_transfer_charge\",\n",
       "    \"57\": \"top_up_by_card_charge\",\n",
       "    \"58\": \"top_up_by_cash_or_cheque\",\n",
       "    \"59\": \"top_up_failed\",\n",
       "    \"60\": \"top_up_limits\",\n",
       "    \"61\": \"top_up_reverted\",\n",
       "    \"62\": \"topping_up_by_card\",\n",
       "    \"63\": \"transaction_charged_twice\",\n",
       "    \"64\": \"transfer_fee_charged\",\n",
       "    \"65\": \"transfer_into_account\",\n",
       "    \"66\": \"transfer_not_received_by_recipient\",\n",
       "    \"67\": \"transfer_timing\",\n",
       "    \"68\": \"unable_to_verify_identity\",\n",
       "    \"69\": \"verify_my_identity\",\n",
       "    \"70\": \"verify_source_of_funds\",\n",
       "    \"71\": \"verify_top_up\",\n",
       "    \"72\": \"virtual_card_not_working\",\n",
       "    \"73\": \"visa_or_mastercard\",\n",
       "    \"74\": \"why_verify_identity\",\n",
       "    \"75\": \"wrong_amount_of_cash_received\",\n",
       "    \"76\": \"wrong_exchange_rate_for_cash_withdrawal\"\n",
       "  },\n",
       "  \"initializer_range\": 2.000E-02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"Refund_not_showing_up\": 0,\n",
       "    \"activate_my_card\": 1,\n",
       "    \"age_limit\": 2,\n",
       "    \"apple_pay_or_google_pay\": 3,\n",
       "    \"atm_support\": 4,\n",
       "    \"automatic_top_up\": 5,\n",
       "    \"balance_not_updated_after_bank_transfer\": 6,\n",
       "    \"balance_not_updated_after_cheque_or_cash_deposit\": 7,\n",
       "    \"beneficiary_not_allowed\": 8,\n",
       "    \"cancel_transfer\": 9,\n",
       "    \"card_about_to_expire\": 10,\n",
       "    \"card_acceptance\": 11,\n",
       "    \"card_arrival\": 12,\n",
       "    \"card_delivery_estimate\": 13,\n",
       "    \"card_linking\": 14,\n",
       "    \"card_not_working\": 15,\n",
       "    \"card_payment_fee_charged\": 16,\n",
       "    \"card_payment_not_recognised\": 17,\n",
       "    \"card_payment_wrong_exchange_rate\": 18,\n",
       "    \"card_swallowed\": 19,\n",
       "    \"cash_withdrawal_charge\": 20,\n",
       "    \"cash_withdrawal_not_recognised\": 21,\n",
       "    \"change_pin\": 22,\n",
       "    \"compromised_card\": 23,\n",
       "    \"contactless_not_working\": 24,\n",
       "    \"country_support\": 25,\n",
       "    \"declined_card_payment\": 26,\n",
       "    \"declined_cash_withdrawal\": 27,\n",
       "    \"declined_transfer\": 28,\n",
       "    \"direct_debit_payment_not_recognised\": 29,\n",
       "    \"disposable_card_limits\": 30,\n",
       "    \"edit_personal_details\": 31,\n",
       "    \"exchange_charge\": 32,\n",
       "    \"exchange_rate\": 33,\n",
       "    \"exchange_via_app\": 34,\n",
       "    \"extra_charge_on_statement\": 35,\n",
       "    \"failed_transfer\": 36,\n",
       "    \"fiat_currency_support\": 37,\n",
       "    \"get_disposable_virtual_card\": 38,\n",
       "    \"get_physical_card\": 39,\n",
       "    \"getting_spare_card\": 40,\n",
       "    \"getting_virtual_card\": 41,\n",
       "    \"lost_or_stolen_card\": 42,\n",
       "    \"lost_or_stolen_phone\": 43,\n",
       "    \"order_physical_card\": 44,\n",
       "    \"passcode_forgotten\": 45,\n",
       "    \"pending_card_payment\": 46,\n",
       "    \"pending_cash_withdrawal\": 47,\n",
       "    \"pending_top_up\": 48,\n",
       "    \"pending_transfer\": 49,\n",
       "    \"pin_blocked\": 50,\n",
       "    \"receiving_money\": 51,\n",
       "    \"request_refund\": 52,\n",
       "    \"reverted_card_payment?\": 53,\n",
       "    \"supported_cards_and_currencies\": 54,\n",
       "    \"terminate_account\": 55,\n",
       "    \"top_up_by_bank_transfer_charge\": 56,\n",
       "    \"top_up_by_card_charge\": 57,\n",
       "    \"top_up_by_cash_or_cheque\": 58,\n",
       "    \"top_up_failed\": 59,\n",
       "    \"top_up_limits\": 60,\n",
       "    \"top_up_reverted\": 61,\n",
       "    \"topping_up_by_card\": 62,\n",
       "    \"transaction_charged_twice\": 63,\n",
       "    \"transfer_fee_charged\": 64,\n",
       "    \"transfer_into_account\": 65,\n",
       "    \"transfer_not_received_by_recipient\": 66,\n",
       "    \"transfer_timing\": 67,\n",
       "    \"unable_to_verify_identity\": 68,\n",
       "    \"verify_my_identity\": 69,\n",
       "    \"verify_source_of_funds\": 70,\n",
       "    \"verify_top_up\": 71,\n",
       "    \"virtual_card_not_working\": 72,\n",
       "    \"visa_or_mastercard\": 73,\n",
       "    \"why_verify_identity\": 74,\n",
       "    \"wrong_amount_of_cash_received\": 75,\n",
       "    \"wrong_exchange_rate_for_cash_withdrawal\": 76\n",
       "  },\n",
       "  \"layer_norm_eps\": 1.000E-12,\n",
       "  \"max_length\": 96,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"padding\": \"max_length\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODELS[\"BERT\"]) \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = prep_model(MODELS[\"BERT\"], config={\"label\": config._num_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=77, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN_LAYERS = 13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args_in = {\n",
    "    \"tokenizer\": tokenizer, \n",
    "    \"train_size\": 100, \n",
    "    \"validation_size\": 0, \n",
    "    \"test_size\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset banking77 (C:/Users/pemma/.cache/huggingface/datasets/banking77/default/1.1.0/ff44c4421d7e70aa810b0fa79d36908a38b87aff8125d002cd44f7fcd31f493c)\n",
      "100%|██████████| 2/2 [00:00<00:00, 240.65it/s]\n",
      "Loading cached processed dataset at C:\\Users\\pemma\\.cache\\huggingface\\datasets\\banking77\\default\\1.1.0\\ff44c4421d7e70aa810b0fa79d36908a38b87aff8125d002cd44f7fcd31f493c\\cache-789d791b63800d76.arrow\n",
      "Loading cached processed dataset at C:\\Users\\pemma\\.cache\\huggingface\\datasets\\banking77\\default\\1.1.0\\ff44c4421d7e70aa810b0fa79d36908a38b87aff8125d002cd44f7fcd31f493c\\cache-f2f24408eb806cc9.arrow\n"
     ]
    }
   ],
   "source": [
    "train_loader_b77, _, test_loader_b77 = load_requested_dataset(config_name=\"b77\", **config_args_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(batch[\"labels\"].shape[0] for batch in train_loader_b77) == config_args_in[\"train_size\"]\n",
    "assert sum(batch[\"labels\"].shape[0] for batch in test_loader_b77) == config_args_in[\"test_size\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args_out = {\n",
    "    \"tokenizer\": tokenizer, \n",
    "    \"train_size\": 0, \n",
    "    \"validation_size\": 0, \n",
    "    \"test_size\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_loader_atis = load_requested_dataset(config_name=\"atis\", **config_args_out)\n",
    "_, _, test_loader_bitext = load_requested_dataset(config_name=\"bitext\", **config_args_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['i want to fly from boston at 838 am and arrive in denver at 1110 in the morning',\n",
       "  'what flights are available from pittsburgh to baltimore on thursday morning',\n",
       "  'what is the arrival time in san francisco for the 755 am flight leaving washington',\n",
       "  'cheapest airfare from tacoma to orlando',\n",
       "  'round trip fares from pittsburgh to philadelphia under 1000 dollars',\n",
       "  'i need a flight tomorrow from columbus to minneapolis',\n",
       "  'what kind of aircraft is used on a flight from cleveland to dallas',\n",
       "  'show me the flights from pittsburgh to los angeles on thursday',\n",
       "  'all flights from boston to washington',\n",
       "  'what kind of ground transportation is available in denver',\n",
       "  'show me the flights from dallas to san francisco',\n",
       "  'show me the flights from san diego to newark by way of houston',\n",
       "  \"what's the airport at orlando\",\n",
       "  'what is the cheapest flight from boston to bwi',\n",
       "  'all flights to baltimore after 6 pm',\n",
       "  'show me the first class fares from boston to denver'],\n",
       " 'labels': ['atis_flight',\n",
       "  'atis_flight',\n",
       "  'atis_flight_time',\n",
       "  'atis_airfare',\n",
       "  'atis_airfare',\n",
       "  'atis_flight',\n",
       "  'atis_aircraft',\n",
       "  'atis_flight',\n",
       "  'atis_flight',\n",
       "  'atis_ground_service',\n",
       "  'atis_flight',\n",
       "  'atis_flight',\n",
       "  'atis_airport',\n",
       "  'atis_flight',\n",
       "  'atis_flight',\n",
       "  'atis_airfare']}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in test_loader_atis: \n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['how can I cancel purchase 113542617735902?',\n",
       "  'can you help me canceling purchase 00004587345?',\n",
       "  'i want assistance to cancel purchase 732201349959',\n",
       "  'i want assistance to cancel order 732201349959',\n",
       "  \"I don't want my last item, help me cancel order 370795561790\",\n",
       "  'I can no longer pay for purchase 00004587345',\n",
       "  'I need assistance cancelling purchase 732201349959',\n",
       "  'is it possible to cancel order 113542617735902?',\n",
       "  'assistance cancelling purchase 00123842',\n",
       "  'i can no longer pay for purchase 113542617735902',\n",
       "  'how tocancel purchase 00004587345',\n",
       "  'i need assistance cancelling order 00123842',\n",
       "  'problems with cancelling purchase 00123842',\n",
       "  'need to cancel order 00004587345',\n",
       "  'I have got to cancel purchase 732201349959',\n",
       "  \"I can't afford this order, cancel purchase 732201349959\"],\n",
       " 'labels': ['cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order',\n",
       "  'cancel_order']}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in test_loader_bitext: \n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass check\n",
    "\n",
    "Evaluate model on one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader_b77: \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['I am still waiting on my card?',\n",
       "  \"What can I do if my card still hasn't arrived after 2 weeks?\",\n",
       "  'I have been waiting over a week. Is the card still coming?',\n",
       "  'Can I track my card while it is in the process of delivery?',\n",
       "  'How do I know if I will get my card, or if it is lost?',\n",
       "  'When did you send me my new card?',\n",
       "  'Do you have info about the card on delivery?',\n",
       "  'What do I do if I still have not received my new card?',\n",
       "  'Does the package with my card have tracking?',\n",
       "  \"I ordered my card but it still isn't here\",\n",
       "  'Why has my new card still not come?',\n",
       "  \"I still haven't received my card after two weeks, is it lost?\",\n",
       "  'Can you track my card for me?',\n",
       "  'Is there a way to track the delivery of my card?',\n",
       "  \"It's been a week since I ordered my card and it's not here. Please help?\",\n",
       "  'Will I be able to track the card that was sent to me?'],\n",
       " 'labels': tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2572,  2145,  3403,  2006,  2026,  4003,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2054,  2064,  1045,  2079,  2065,  2026,  4003,  2145,  8440,\n",
       "          1005,  1056,  3369,  2044,  1016,  3134,  1029,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1045,  2031,  2042,  3403,  2058,  1037,  2733,  1012,  2003,\n",
       "          1996,  4003,  2145,  2746,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2064,  1045,  2650,  2026,  4003,  2096,  2009,  2003,  1999,\n",
       "          1996,  2832,  1997,  6959,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2129,  2079,  1045,  2113,  2065,  1045,  2097,  2131,  2026,\n",
       "          4003,  1010,  2030,  2065,  2009,  2003,  2439,  1029,   102,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2043,  2106,  2017,  4604,  2033,  2026,  2047,  4003,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2079,  2017,  2031, 18558,  2055,  1996,  4003,  2006,  6959,\n",
       "          1029,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2054,  2079,  1045,  2079,  2065,  1045,  2145,  2031,  2025,\n",
       "          2363,  2026,  2047,  4003,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2515,  1996,  7427,  2007,  2026,  4003,  2031,  9651,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1045,  3641,  2026,  4003,  2021,  2009,  2145,  3475,  1005,\n",
       "          1056,  2182,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2339,  2038,  2026,  2047,  4003,  2145,  2025,  2272,  1029,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  1045,  2145,  4033,  1005,  1056,  2363,  2026,  4003,  2044,\n",
       "          2048,  3134,  1010,  2003,  2009,  2439,  1029,   102,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2064,  2017,  2650,  2026,  4003,  2005,  2033,  1029,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2003,  2045,  1037,  2126,  2000,  2650,  1996,  6959,  1997,\n",
       "          2026,  4003,  1029,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2009,  1005,  1055,  2042,  1037,  2733,  2144,  1045,  3641,\n",
       "          2026,  4003,  1998,  2009,  1005,  1055,  2025,  2182,  1012,  3531,\n",
       "          2393,  1029,   102],\n",
       "        [  101,  2097,  1045,  2022,  2583,  2000,  2650,  1996,  4003,  2008,\n",
       "          2001,  2741,  2000,  2033,  1029,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer #0: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #1: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #2: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #3: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #4: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #5: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #6: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #7: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #8: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #9: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #10: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #11: (n_samples=16, seq_len=23, embed_size=768)\n",
      "Layer #12: (n_samples=16, seq_len=23, embed_size=768)\n"
     ]
    }
   ],
   "source": [
    "for i, h in enumerate(output.hidden_states):\n",
    "    n_samples, seq_len, embed_size = h.shape \n",
    "    print(f\"Layer #{i}: ({n_samples=}, {seq_len=}, {embed_size=})\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MahalanobisScorer]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_selection = {\"last\": [-1], \"all\": [l for l in range(N_HIDDEN_LAYERS)]}\n",
    "\n",
    "detectors: List[ScorerType] = [MahalanobisScorer(layers=layer_selection[\"last\"])]\n",
    "detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score_names': [],\n",
       " 'chosen_state': 'encoder_hidden_states',\n",
       " 'use_first_token_only': True,\n",
       " 'accumulated_embeddings': defaultdict(list, {}),\n",
       " 'layers': {-1},\n",
       " 'accumulation_device': 'cpu',\n",
       " 'covs': None,\n",
       " 'means': None}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = detectors[0]\n",
    "detector.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = prepare_detectors(detectors, model, train_loader_b77, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score_names': ['layer_-1_class_0'],\n",
       " 'chosen_state': 'encoder_hidden_states',\n",
       " 'use_first_token_only': True,\n",
       " 'layers': {-1},\n",
       " 'accumulation_device': 'cpu',\n",
       " 'covs': {(-1,\n",
       "   0): tensor([[ 0.0264,  0.0017,  0.0093,  ..., -0.0129,  0.0114, -0.0217],\n",
       "          [ 0.0017,  0.0285,  0.0090,  ...,  0.0087, -0.0068, -0.0072],\n",
       "          [ 0.0093,  0.0090,  0.0496,  ...,  0.0096,  0.0123, -0.0104],\n",
       "          ...,\n",
       "          [-0.0129,  0.0087,  0.0096,  ...,  0.0563, -0.0001,  0.0206],\n",
       "          [ 0.0114, -0.0068,  0.0123,  ..., -0.0001,  0.0399, -0.0087],\n",
       "          [-0.0217, -0.0072, -0.0104,  ...,  0.0206, -0.0087,  0.0711]])},\n",
       " 'means': {(-1,\n",
       "   0): tensor([ 2.2000e-01,  2.6586e-01,  1.1547e+00, -1.4799e-01,  1.8522e-01,\n",
       "           4.1743e-01, -4.7661e-01,  3.4912e-01, -5.3622e-01, -3.5810e-01,\n",
       "           4.3992e-01, -8.1078e-01, -1.3262e+00,  8.3212e-02, -1.4752e-01,\n",
       "          -8.7662e-03,  1.9318e-01,  3.7039e-01, -2.6248e-01,  7.9264e-01,\n",
       "          -3.3909e-01, -6.1834e-01,  4.5298e-02,  5.3958e-02,  5.2791e-01,\n",
       "          -6.5677e-01, -4.4591e-01, -3.0122e-01, -7.9606e-01,  2.1457e-01,\n",
       "          -1.0539e+00, -5.0496e-02,  2.2875e-01,  1.0854e+00,  8.6490e-01,\n",
       "           4.9087e-01,  5.5732e-01, -1.1158e+00, -5.3777e-01, -1.6786e-01,\n",
       "          -9.4247e-01, -6.5879e-01,  4.0747e-01,  2.4949e-01, -6.9202e-01,\n",
       "          -5.4864e-01, -4.3217e-01,  1.0498e+00, -5.6011e-01, -2.1402e-01,\n",
       "          -8.5433e-01, -1.2956e-01, -8.6661e-01,  6.9011e-01,  2.8264e-01,\n",
       "          -2.6641e-01,  4.4012e-01, -7.7244e-01, -9.9558e-01, -5.2794e-01,\n",
       "          -7.6721e-01, -4.1380e-01,  3.6262e-02, -5.1173e-01,  3.7439e-01,\n",
       "           2.0366e-01,  7.7876e-01,  8.9396e-01,  4.7287e-01, -7.8249e-02,\n",
       "           6.8768e-01, -3.0417e-01,  1.1851e-01, -1.6000e-03, -3.2195e-02,\n",
       "          -5.7078e-01, -8.7853e-02,  1.6608e+00,  8.1521e-01,  3.0863e-01,\n",
       "          -1.7662e+00,  6.8182e-01, -1.6405e-01, -4.1111e-01,  4.0567e-01,\n",
       "           3.2156e-01,  9.4577e-02, -2.4849e-01,  3.8676e-01, -8.7997e-01,\n",
       "           6.0076e-01, -1.4548e+00,  4.1766e-01,  6.7918e-01, -1.2583e-01,\n",
       "          -3.1572e-01,  1.5359e-01,  9.7543e-01, -8.0214e-01,  6.0973e-02,\n",
       "           2.1878e-01, -2.7451e-01,  5.2177e-01, -3.2345e-01,  2.5289e-01,\n",
       "          -8.5442e-01,  3.8892e-01,  1.1123e-01,  8.3965e-01,  2.4344e-01,\n",
       "           5.4399e-01, -1.3681e-01,  1.5120e-01, -1.3890e+00, -1.6174e-01,\n",
       "          -4.1103e-01,  3.9891e-01, -9.5093e-01,  1.3365e-01,  3.1620e-01,\n",
       "           1.5707e-01, -4.1453e-01,  7.9596e-01, -4.7509e-01,  3.4127e-01,\n",
       "          -9.2788e-02,  4.6831e-01,  3.0441e-01,  5.3680e-01, -2.4549e-01,\n",
       "           4.6710e-01,  1.0442e-01,  7.1246e-01,  9.2489e-01, -3.8935e-02,\n",
       "           1.1287e+00, -1.1536e+00, -9.4704e-01,  7.0529e-01,  1.4523e+00,\n",
       "          -8.2209e-01,  5.4658e-02,  3.4202e-01,  3.5366e-01,  2.5036e-01,\n",
       "           9.2206e-01,  1.2917e-01, -2.8150e-01,  7.2062e-01, -3.8461e-01,\n",
       "           5.2324e-01, -2.5079e-01, -4.8176e-01, -2.0632e-01, -9.6033e-01,\n",
       "          -9.7587e-01,  1.6450e-01,  4.0848e-01,  7.4305e-02, -4.4109e-01,\n",
       "           3.4982e-01,  5.6089e-01, -6.6094e-01,  4.3632e-01, -3.9791e-01,\n",
       "          -3.1414e-01,  3.2306e-01,  3.5328e-01,  3.8165e-01,  1.4839e-01,\n",
       "           1.6809e-01, -9.1588e-01,  1.1757e+00,  5.4194e-01, -5.3970e-01,\n",
       "          -1.3014e+00, -4.3958e-01,  1.7594e-01,  2.4811e-01, -1.3422e-01,\n",
       "           4.1357e-01,  5.3337e-01, -3.9915e-01,  7.6319e-01, -5.0036e-01,\n",
       "          -4.8669e-01, -3.1982e-02, -8.5853e-01, -1.7716e+00, -2.0127e-01,\n",
       "           6.0501e-01, -8.7413e-01,  2.2878e-02,  2.5672e-01,  3.0869e-01,\n",
       "          -9.7559e-02, -3.1023e-01,  6.6838e-01, -4.1655e-02, -2.7095e-01,\n",
       "           5.7954e-01, -8.8570e-01, -1.5635e-01,  1.0108e+00,  4.5946e-01,\n",
       "          -9.6056e-03, -8.6235e-02, -1.0065e+00, -2.9367e-01,  6.2042e-02,\n",
       "          -5.3590e-01,  2.8979e-01, -7.2700e-02,  2.4720e-01, -6.2141e-01,\n",
       "          -1.4704e-01,  8.1871e-01, -1.1124e+00, -3.3174e-01,  1.3270e+00,\n",
       "           3.0658e-01, -3.8487e-01,  2.6564e-01,  1.1469e+00,  3.2050e-01,\n",
       "           9.0611e-01,  1.3803e+00, -3.0397e-01, -2.4321e-01,  2.5355e-01,\n",
       "          -3.1620e-01, -6.5111e-01, -1.9393e-01, -3.8607e-01, -8.7951e-01,\n",
       "          -4.9033e-01, -3.1420e-01,  1.5668e-01, -1.4755e-01, -4.3870e-01,\n",
       "           4.6086e-01,  3.5530e-01, -5.6962e-01,  7.3682e-01,  6.0646e-01,\n",
       "          -3.6613e-01, -5.2548e-01,  1.9272e-01, -1.9574e-01, -1.0571e+00,\n",
       "          -1.2436e+00,  2.5264e-01, -3.3221e-02,  1.3886e-01,  3.6966e-01,\n",
       "           2.6692e-02, -1.8623e-01,  1.1228e+00,  1.5327e+00,  3.0188e-01,\n",
       "          -5.6371e-01, -5.4215e-01,  2.1193e-01,  7.5841e-01, -6.5792e-01,\n",
       "           3.2035e-01, -7.1632e-01, -5.9504e-01,  3.6985e-01, -2.3026e-01,\n",
       "          -5.2536e-01, -4.2920e-01, -7.9357e-01,  3.9882e-01,  7.8558e-01,\n",
       "          -4.9213e-01, -3.1948e-01,  5.8899e-02, -3.7668e-01, -7.4693e-01,\n",
       "           5.4858e-01,  1.3696e+00,  2.5975e-01, -9.5889e-01, -7.3319e-01,\n",
       "          -5.0778e-01, -1.0469e-01, -9.8692e-04,  6.2303e-01, -1.0883e+00,\n",
       "          -8.7537e-01, -3.0839e-01,  3.7180e-01,  2.6039e-01,  5.8111e-01,\n",
       "          -4.6064e-01,  5.1861e-01,  4.4544e-01, -3.9755e-01, -4.6718e-01,\n",
       "           3.1217e-01, -1.0261e+00, -1.5945e-01, -5.1289e-02, -5.0514e-01,\n",
       "           4.6652e-01, -7.6657e-02, -7.0878e-01,  3.5914e-02, -1.3614e-01,\n",
       "           1.6665e-01, -2.5239e-01, -6.2889e-01, -2.1552e-02,  7.4223e-01,\n",
       "           5.0427e-02,  7.5703e-01, -1.3045e+00,  3.9252e-01,  5.3330e-01,\n",
       "           3.4191e-01, -9.3893e-01,  2.9629e-01, -3.6115e-01, -1.1819e+00,\n",
       "          -1.1717e-01,  9.6053e-01,  3.7511e-01,  2.7454e-01, -6.9786e-01,\n",
       "          -7.7111e-01,  1.2635e+00,  8.5646e-01,  8.9015e-01, -1.5214e-01,\n",
       "          -2.7048e-01,  1.6982e-01, -1.8556e-01, -1.5823e-01, -9.9073e-01,\n",
       "           5.5208e-01, -4.1139e-01, -1.0598e+00,  4.6857e-01, -2.6937e-02,\n",
       "           7.3771e-01,  4.1938e-02, -5.0514e-01, -1.0225e+00,  2.0392e-01,\n",
       "          -1.1178e-01, -7.2085e-01,  3.4995e-01,  3.4457e-01, -5.4794e-01,\n",
       "          -5.9918e-01, -2.6400e-01,  3.2009e-01, -1.8006e-01,  4.4059e-01,\n",
       "          -1.4263e-02, -7.6158e-01, -8.2417e-02,  5.3282e-01, -2.6838e-01,\n",
       "          -9.1901e-01,  4.2409e-01, -5.7526e-01,  1.9571e-01,  3.7885e-01,\n",
       "          -8.7646e-01,  9.3637e-01, -7.6143e-01, -8.4802e-01,  1.3205e-01,\n",
       "           2.2835e-01,  1.5216e-01, -5.1117e-01,  3.1261e-01, -1.0047e+00,\n",
       "          -3.2602e-01,  5.0161e-02, -8.5831e-01, -9.3107e-01, -7.4487e-01,\n",
       "          -2.5359e-02,  7.2950e-01,  1.6002e-01, -1.2389e+00, -1.1097e+00,\n",
       "          -9.4345e-01, -9.3044e-01,  2.9316e-01,  1.6569e-01,  5.6200e-01,\n",
       "           4.5314e-01, -2.2654e-01, -5.5720e-01,  7.0156e-01, -1.1136e+00,\n",
       "          -8.3153e-02,  3.4552e-01,  1.1531e+00, -8.1451e-02,  6.5561e-01,\n",
       "          -1.6260e-02, -2.0756e-01,  1.2884e+00, -3.2236e-01,  8.1956e-01,\n",
       "           3.4463e-01, -1.0411e+00,  4.4469e-01,  8.4655e-01, -1.1051e+00,\n",
       "           9.7698e-02,  2.2451e-01,  3.7063e-01,  1.0880e+00, -8.0880e-01,\n",
       "           3.4700e-01, -1.3705e-01,  3.1602e-01, -5.0373e-01,  4.2408e-01,\n",
       "           3.5100e-01, -8.4711e-01, -1.0958e-01,  5.0390e-01, -1.8388e-01,\n",
       "          -9.9168e-01,  8.1479e-01, -3.7511e-01, -4.4544e-01, -1.2181e+00,\n",
       "          -1.0494e+00,  7.4641e-01, -1.0081e+00,  5.6117e-01,  1.9687e-01,\n",
       "          -4.1852e-01,  9.2265e-01,  7.4208e-01,  1.4619e+00,  7.9676e-01,\n",
       "          -1.9329e-01, -7.5491e-02, -5.7482e-01, -7.8985e-02,  1.6105e-01,\n",
       "           4.4573e-01, -1.9618e-01,  5.6711e-01,  2.5118e-01, -4.7601e-02,\n",
       "          -6.1290e-01, -2.5652e-02,  1.6699e-01, -9.2354e-01, -3.4814e-01,\n",
       "           8.4186e-01, -2.5711e-01, -9.4564e-01,  9.0111e-02,  2.9486e-01,\n",
       "           5.1710e-01,  2.7763e-01, -1.8729e-01,  4.5840e-01, -5.9176e-01,\n",
       "           2.0421e-01,  7.4403e-01, -4.5820e-01,  8.0077e-01,  2.3079e-01,\n",
       "           4.3385e-02, -5.6994e-01,  2.3018e-02, -2.1004e-01, -7.5355e-01,\n",
       "          -7.0895e-01, -8.6163e-01, -2.5159e-01,  5.2503e-01,  2.6888e-01,\n",
       "           9.8764e-04,  2.4881e-01,  3.4307e-01, -3.7235e-01, -5.8969e-01,\n",
       "          -9.6000e-01,  4.4447e-01,  1.9743e-01,  1.0452e+00,  9.0744e-01,\n",
       "          -7.1933e-01, -4.8176e-01, -4.6784e-01,  2.3423e-01,  5.6026e-01,\n",
       "          -7.7165e-01, -5.2541e-01, -8.5639e-01,  8.3584e-01, -1.3822e-01,\n",
       "          -9.4003e-01,  1.1612e+00,  5.8400e-02, -1.9158e-02, -9.7265e-02,\n",
       "          -8.2595e-01,  5.7813e-01, -4.1074e-01,  3.3225e-01,  2.3055e-02,\n",
       "          -1.0584e+00, -8.8556e-01, -2.6279e-01, -8.6474e-01, -1.9422e-01,\n",
       "           1.7215e-01, -2.5279e-01, -2.4399e-01,  2.2361e-01,  4.4165e-02,\n",
       "          -1.0161e+00, -5.0127e-02,  2.9758e-02, -6.6178e-01,  3.6688e-01,\n",
       "           7.7482e-02, -5.7360e-01, -7.4165e-01, -3.7612e-01,  6.0988e-01,\n",
       "           6.9824e-02, -5.5876e-01,  1.7350e-01,  9.0894e-01, -7.9835e-01,\n",
       "          -2.6617e-01, -1.0769e-01, -2.4998e-01,  1.1345e+00,  1.3505e-03,\n",
       "          -4.4248e-01,  1.5770e-01,  1.6752e-01, -3.4767e-01,  5.3540e-01,\n",
       "           4.8508e-01,  7.1861e-01, -4.8288e-02,  8.0894e-01, -9.2471e-02,\n",
       "          -3.4325e-01,  7.0315e-01,  4.4701e-02,  8.1193e-01,  8.7937e-01,\n",
       "          -4.8728e-02,  1.8370e+00,  8.1848e-01,  3.5816e-01, -2.2440e-01,\n",
       "           1.6865e-02, -9.3769e-01,  3.4609e-01,  6.3704e-01, -9.2945e-01,\n",
       "           2.8693e-01,  2.4678e-01,  8.3075e-01, -8.0513e-01, -1.0173e+00,\n",
       "           1.5017e-01, -6.4493e-02, -9.0439e-01, -5.2068e-01,  3.9365e-01,\n",
       "           2.2271e-01, -3.7407e-01,  1.0969e-01, -6.1036e-01,  4.5324e-01,\n",
       "          -4.0107e-01, -5.6828e-02, -3.4823e-03,  3.7945e-02, -4.1287e-01,\n",
       "          -2.2680e-01, -1.0339e+00,  5.9889e-01, -1.0741e+00, -6.8788e-01,\n",
       "          -4.5269e-01, -3.7228e-01,  2.3651e-01,  1.4978e-01,  2.3463e-01,\n",
       "          -3.4760e-01,  8.4000e-02,  7.2656e-01, -2.8562e-02, -3.2424e-01,\n",
       "           9.9460e-01,  5.0361e-02, -8.9913e-02,  4.2608e-01, -2.0068e-01,\n",
       "           3.3174e-01,  4.7350e-01, -6.0307e-01,  4.6025e-01, -2.2189e-01,\n",
       "          -2.2756e+00, -9.8933e-01, -2.1633e-01, -9.9399e-01, -8.9441e-01,\n",
       "           1.7539e+00,  6.0327e-01, -8.9504e-01,  5.0952e-01,  1.8301e-01,\n",
       "           6.5657e-01, -1.2594e+00, -5.7350e-02,  4.3630e-01,  1.3885e-01,\n",
       "          -6.3274e-02,  1.0422e+00,  9.7048e-01, -8.2355e-02, -9.0220e-01,\n",
       "          -4.0893e-01, -7.4930e-02,  7.0509e-01, -1.5263e-01, -3.3346e-01,\n",
       "          -3.7391e-01,  3.1383e-01,  1.1676e+00, -9.2705e-01,  1.8368e+00,\n",
       "           5.1806e-01,  1.7810e-01,  2.9503e-01,  6.6914e-01,  1.5462e-01,\n",
       "           3.8158e-01,  1.0125e+00,  5.5172e-01, -1.3621e-01,  2.5262e-01,\n",
       "           1.2852e-01, -4.8932e-01, -8.9055e-01,  1.0108e+00, -8.8942e-01,\n",
       "          -1.6155e-01,  3.9724e-01, -8.6764e-03, -2.6242e-01, -1.0321e+00,\n",
       "          -2.5965e-02, -3.0918e-01,  4.8558e-01, -1.1075e+00,  5.9871e-02,\n",
       "           5.9636e-01,  1.0095e-01, -3.0101e-02,  3.8711e-01,  9.9630e-01,\n",
       "          -6.0339e-01,  1.1006e-01, -3.3462e-01, -4.9047e-01, -1.5498e-01,\n",
       "           8.7942e-01,  8.0398e-01, -6.2221e-02,  1.3216e-01,  3.9124e-01,\n",
       "           3.3873e-01, -3.1319e-01, -9.5342e-01, -1.2231e+00,  2.2344e-03,\n",
       "           6.1459e-02,  1.0618e+00, -5.7046e-01, -5.5921e-01, -7.4741e-01,\n",
       "           2.9550e-01,  3.1087e-01, -1.5158e+00,  1.3806e+00,  8.2649e-02,\n",
       "          -3.2875e-01, -1.1412e-01, -1.0476e+00, -1.8135e-01,  6.8424e-03,\n",
       "           3.5412e-01,  1.2315e-01, -1.7560e-01, -1.2461e+00, -7.9942e-02,\n",
       "          -5.8412e-02,  7.2487e-01,  4.1039e-01, -9.7158e-01,  4.9477e-01,\n",
       "           4.7139e-03,  7.2338e-01, -6.0018e-02,  1.2341e-01,  2.8279e-01,\n",
       "          -1.0800e+00, -7.0145e-01,  7.1086e-02,  3.5698e-01, -5.8763e-01,\n",
       "           4.0325e-01, -1.2873e-01,  3.3827e-01, -5.2704e-01,  9.2093e-01,\n",
       "           1.6575e+00,  7.2009e-01, -4.1888e-03,  9.8035e-01,  1.1095e+00,\n",
       "           2.6459e-01, -7.9796e-01, -3.0050e-01,  2.0525e-01,  9.2702e-01,\n",
       "           4.4189e-01,  2.4932e-01,  3.4638e-01,  1.2278e-01, -2.8568e-01,\n",
       "          -2.6482e-01,  5.4368e-01, -2.9539e-01,  3.6023e-01, -1.6022e-01,\n",
       "           3.5274e-01,  4.0525e-01, -3.6887e-02,  8.1383e-01,  5.4326e-01,\n",
       "          -3.6905e-02, -9.1353e-01, -6.0349e-02,  1.6773e-01,  3.4252e-01,\n",
       "           2.3290e-01, -8.6211e-01,  4.2838e-01,  3.8174e-01,  4.2216e-01,\n",
       "          -7.5571e-01,  1.1636e-01, -7.6267e-01])}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector = detectors[0]\n",
    "detector.__dict__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the in-distribution training set\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on the in-distribution training set\")\n",
    "\n",
    "records_in_train = evaluate_dataloader(\n",
    "    model,\n",
    "    train_loader_b77,\n",
    "    tokenizer,\n",
    "    detectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MahalanobisScorer(layers={-1},use_first_token_only=True,chosen_state=encoder_hidden_states)+layer_-1_class_0', 'likelihood', 'pred_label', 'true_label', 'correct']\n"
     ]
    }
   ],
   "source": [
    "keys = list(records_in_train.keys()) \n",
    "print(keys)\n",
    "\n",
    "score_name = keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the in-distribution test set\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on the in-distribution test set\")\n",
    "\n",
    "records_in_test = evaluate_dataloader(\n",
    "    model,\n",
    "    train_loader_b77,\n",
    "    tokenizer,\n",
    "    detectors\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the out-of-distribution ATIS dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating detectors...: 100%|██████████| 7/7 [00:09<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on the out-of-distribution ATIS dataset\")\n",
    "\n",
    "records_out_atis = evaluate_dataloader(\n",
    "    model,\n",
    "    test_loader_atis,\n",
    "    tokenizer,\n",
    "    detectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the out-of-distribution BITEXT dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating detectors...: 100%|██████████| 7/7 [00:12<00:00,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on the out-of-distribution BITEXT dataset\")\n",
    "\n",
    "records_out_bitext = evaluate_dataloader(\n",
    "    model,\n",
    "    test_loader_bitext,\n",
    "    tokenizer,\n",
    "    detectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_names = [\n",
    "    \"b77_train\", \n",
    "    \"b77_test\", \n",
    "    \"atis\", \n",
    "    \"bitext\"\n",
    "]\n",
    "\n",
    "records = [\n",
    "    records_in_train, \n",
    "    records_in_test, \n",
    "    records_out_atis, \n",
    "    records_out_bitext\n",
    "]\n",
    "\n",
    "maha_scores = pd.DataFrame({\n",
    "    ds_name: records[score_name]\n",
    "    for ds_name, records in zip(ds_names, records)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "fig.suptitle(u\"MahalanobisScorer - IN & OUT datasets \\n Mahalanobis distance ($d_M$) from training set\")\n",
    "\n",
    "sns.histplot(data=maha_scores, ax=ax, bins=50) \n",
    "ax.set_xlabel(r\"$d_M$\"); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
